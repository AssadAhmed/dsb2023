---
title: "Homework 2"
author: "Assad Ahmed"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
library(ggforce)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


mass_shootings <- read_csv(here::here("data", "mass_shootings.csv"))

glimpse(mass_shootings)

head(mass_shootings)
```

| column(variable)     | description                                                                 |
|--------------------------|----------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}
#Create a new dataframe for mass shootings per year called mass_shootings_yearly

mass_shootings_yearly<- mass_shootings %>% 
  #Filter out any NA's
  filter(!is.na(year)) %>% 
  #group by year and summarize the count of mass shootings
  group_by(year) %>% 
  summarize(Count = n())

mass_shootings_yearly
```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}
#Creating a barchart for number of mass shooters with each race catagory
mass_shootings %>% 
  #replace NA race entries with 'Unknown'
  #reorder from most to least frequent race
  mutate(race= case_when( !is.na(race)  ~ race, TRUE ~ 'Other'),race=fct_infreq(race)) %>%
  #use GGplot to produce a bar char, initialse x variable and aes
  ggplot(aes(x=race)) +
  #plot on a barchart
  geom_bar() +
  #add asthetic label
  geom_text(aes(label = after_stat(count)), stat = "count", vjust = 1.1, colour = "white")
```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r}
#assume total victims means to exclude the shooter from the total
#manipulate data for the boxplot
mass_shootings %>% 
  #Filter out for victim count NA
  filter(!is.na(total_victims)) %>% 
  #replace NA race entries with 'Unknown'
  mutate(location= case_when( !is.na(location_type)  ~ location_type, TRUE ~ 'Other')) %>%
  #Create a plot
  ggplot(aes(x=location_type,y=total_victims)) +
  geom_boxplot() +
  #facet zoom to better see the plots on a more reasonable scale due to the outlier
  facet_zoom(ylim = c(0, 150))
```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

```{r}
#View data set for las vegas strip massacre only
mass_shootings %>% 
  #only filter for las vegas strip
  filter(grepl("Las Vegas Strip",case))

#only shows the 1 entry as required

#assume total victims means to exclude the shooter from the total
#manipulate data for the boxplot
mass_shootings %>% 
  #Filter out for victim count NA and for the las vegas strip massacre
  filter(!is.na(total_victims) & !grepl("Las Vegas Strip",case)) %>% 
  #replace NA race entries with 'Unknown'
  mutate(location= case_when( !is.na(location_type)  ~ location_type, TRUE ~ 'Other')) %>%
  #Create a plot
  ggplot(aes(x=location_type,y=total_victims)) +
  geom_boxplot()
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}
<<<<<<< HEAD
Prior_Mental_2000 <- mass_shootings %>% 
  #Filter for year>= 2000, for white males only
  filter (year>=2000 & race=='White' & male==TRUE) %>% 
  #Convert NA's to a new category 'Unknown'
  #this will allow us to see what proportion of mass shooters have a hidden history in terms of mental     illness which will help to validate/invalidate any conclusions
  mutate(prior_mental_illness= case_when( !is.na(prior_mental_illness)  
                                ~ prior_mental_illness, TRUE ~ 'Unknown'),
         #add an arbitary weighting clumn
         n=1, 
         #add column calculating the % share each case has to the total based on current equal weighting
         percent=n/sum(n)) %>%
  #grup by prior_mental_illness
  group_by(prior_mental_illness) %>% 
  #summarise data for count of each
  summarize(Count = n(),
            percent = sum(percent))

Prior_Mental_2000
#>50% of white males had a history of prior mental illness since 2000, however >40% also had an unknown history
#hence it would be  be unreliable to draw a conclusion from the data, given the unknoen portion representa a significant portion of the data.

#instead lets exclude the unknowns and see of the cases with known mental health histry what the proportions are if we assume the unknonwn data behaviours can be assimilated by the behaviour ofthe known dataset

Prior_Mental_2000_v2 <- mass_shootings %>% 
  #Filter for year>= 2000, for white males only & exlcude NA values in prior mental ilness
  filter (year>=2000 & race=='White' & male==TRUE & !is.na(prior_mental_illness)) %>% 
  mutate(        #add an arbitary weighting clumn
         n=1, 
         #add column calculating the % share each case has to the total based on current equal weighting
         percent=n/sum(n)) %>%
  #grup by prior_mental_illness
  group_by(prior_mental_illness) %>% 
  #summarise data for count of each
  summarize(Count = n(),
            percent = sum(percent))

Prior_Mental_2000_v2

#of cases with a known history of mental illness for white males since 2000, almost 85% were perpetrated by individuals with a history of mental health issues.
#We could extrapolate this relation ship and assume the 85% of cases that have a history of prior mental ilness holds

#overall i belive the prior_mental_illness field has too many NA values to produce a strong conclusion on the relationship between cases and mental ilness. However, we can clearly see that a large proportion of cases (>50% at the minimum) show a history of mental illness which shows it is a significant factor, but a valid conclusion on the full extent of the relationship cannot be understood due to the limited data.


=======
Prior_Mental_200 <- mass_shootings %>% 
  #Convert NA's to a new category 'Unknown'
>>>>>>> 17fcd9893772743f9ef243d36278466a3ee5f775
```

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

```{r}

mass_shootings %>% 
  #Filter for any month data missing
  filter(!is.na(month)) %>% 
  #Group by month
  group_by(month) %>% 
  #create bar chart
  ggplot(aes(x=month)) +
  geom_bar() + 
  #order chronologically
  scale_x_discrete(limits = month.abb)
  
#It can clearly be seen from the bar chart that February is the most dangerous month in term sof number of shootings

```

-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

```{r}
#Create list of races to be compared
R <- c('Latino','White', 'Black')

#Create a range of bar charts to compare distribution by month
mass_shootings %>% 
  #Filter for any total victims data missing
  #Exclude las vegas strip case as it is a large outlier which will inflate the mean
  filter(!is.na(total_victims) & race %in% R & !grepl("Las Vegas Strip",case)) %>% 
  #create box plot as it is easy to compare IQR and means side by side
  ggplot(aes(x=race,y=total_victims)) +
  geom_boxplot() +
  facet_zoom(ylim = c(0, 30)) 
#Black shooters have a lower median and narrower distribution of victims as compared to white shooters
#Lation shooters also have a lower median number of victims than white shooters, and a significantly narrower IQR vs white shooters.
#Overall white shooters appear to have a larger number of victims on average, with mass shootings from white shooters covering a wider range of vitim totals as comapred to Balck/Lation

### Very open-ended

#-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no #signs of mental illness in the shooter?
  
#utilise dataframe created earlier comparing Yes/No mental health issues, excuding cases where mental health histroy was unknown

mass_shootings %>% 
  #create unknown catagory for NA's
  mutate(prior_mental_illness= case_when(!is.na(prior_mental_illness) ~ prior_mental_illness, TRUE ~ 'Unknown')) %>% 
  #group by prior_mental_illness
  group_by(prior_mental_illness) %>% 
  ggplot(aes(x=prior_mental_illness,y=total_victims)) +
  geom_boxplot() +
  facet_zoom(ylim = c(0, 30)) 

#From the limited data we have on prior mental ilness, we can clearly see that shootings with a history of mental ilness for the shooter on average have a higher total victim count, and a significantly wider range/IQR in the total number of victimis Vs those without a history of mental illness.
#Again however,we can see from the plot of the cases with an unknown histroy of mental illness that they exhibit a lower median and intermediate IQR when comapred to Yes/No, which could skew the conclusion depending on how those unknown datapoints are catagorised. 

```{r}

```

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

```{r}
#Mental ilness and total victims comparison
mass_shootings %>% 
  #create unknown catagory for NA's
  mutate(prior_mental_illness= case_when(!is.na(prior_mental_illness) ~ prior_mental_illness, TRUE ~ 'Unknown')) %>% 
  #group by prior_mental_illness
  group_by(prior_mental_illness) %>% 
  ggplot(aes(x=prior_mental_illness,y=total_victims)) +
  geom_boxplot() +
  facet_zoom(ylim = c(0, 30)) 

#Explanation from previous
#From the limited data we have on prior mental ilness, we can clearly see that shootings with a history of mental ilness for the shooter on average have a higher total victim count, and a significantly wider range/IQR in the total number of victimis Vs those without a history of mental illness.
#Again however,we can see from the plot of the cases with an unknown histroy of mental illness that they exhibit a lower median and intermediate IQR when comapred to Yes/No, which could skew the conclusion depending on how those unknown datapoints are catagorised. 


#Mental illness and location type comparison
mass_shootings %>% 
  #create unknown catagory for NA's
  mutate(prior_mental_illness= case_when(!is.na(prior_mental_illness) ~ prior_mental_illness, TRUE ~ 'Unknown')) %>% 
  #group by prior_mental_illness
  group_by(location_type, prior_mental_illness) %>% 
  #Summarise count for each catagory
  summarise(count=n()) %>% 
  #multiple bar chart
  ggplot(aes(location_type, count, fill = prior_mental_illness)) +
  geom_bar(stat="identity", position = "dodge")

#consistent trend with prior mental illness producing more cases in each location, except in the military where interestingly unknown history of mental illness was the most cases.

#intersection of mental ilness, location, total victims comparison
mass_shootings %>% 
  #create unknown catagory for NA's
  mutate(prior_mental_illness= case_when(!is.na(prior_mental_illness) ~ prior_mental_illness, TRUE ~ 'Unknown')) %>% 
  #group by prior_mental_illness
  group_by(location_type, prior_mental_illness) %>% 
  #multiple bar chart
  ggplot(aes(location_type, total_victims, fill = prior_mental_illness)) +
  geom_bar(stat="identity", position = "dodge") +
  facet_zoom(ylim = c(0, 100)) 

#a consistent trend across all location with a history of mental illness leading to a higher total victim count Vs no history of mental illness.
#Additonally, cases where prior mental health history is unknown also appear to have a high total victim count which could imply a significant portion of that population may also have a history of mental illness. Although, there is not enough data/information on the data to draw such a conclusion currently.
```


# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
head(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}

card_fraud %>% 
  #Filter out transactions without a fraud flag
  filter(!is.na(is_fraud)) %>% 
  #Group by transaction year
  group_by(trans_year) %>% 
  #add a count of transactions and calculate frequency as total count/365 days
  summarise(No_Frauds = n(),
            Freq_Fraud_per_day = No_Frauds/365,
            Freq_per_mins= No_Frauds/(365*24*60))
```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

```{r}

Summary_One <- card_fraud %>% 
  #Filter out transactions without a fraud flag or amt
  filter(!is.na(is_fraud) & !is.na(amt)) %>% 
  #mutate 2 new columns for fraudulent/none fraudulent transaction amt in USD
  mutate(Amt_Non_Fraudulent = case_when(is_fraud==1 ~ 0, TRUE ~ amt),
         Amt_Fraudulent = case_when(is_fraud == 0 ~ 0, TRUE ~ amt)) %>% 
  #Group by transaction year
  group_by(trans_year) %>% 
  #summaries total amount for non-fraudulent, fraudulent transactions, and the % share for fraudulent transactions in term sof value
  summarise(Total_Non_Fraudulent = sum(Amt_Non_Fraudulent),
            Total_Fraudulent = sum(Amt_Fraudulent),
            percent_fraud= Total_Fraudulent/(Total_Non_Fraudulent+Total_Fraudulent))

Summary_One

```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}

card_fraud %>% 
  #Filter out transactions without a fraud flag or amt
  filter(!is.na(is_fraud) & !is.na(amt) ) %>% 
  #Group by is fraud
  group_by(is_fraud) %>% 
  #Create histogram plot
  ggplot(aes(x=amt)) +
  geom_histogram() +
  facet_wrap(~is_fraud, scales = "free") 

#Summary statistics

#Summary for fraudulent
skim(card_fraud %>% 
  #Filter out transactions without a fraud flag or amt
  filter(!is.na(is_fraud) & !is.na(amt) & is_fraud==1 ))

#Summary for Non-fraudulent
skim(card_fraud %>% 
  #Filter out transactions without a fraud flag or amt
  filter(!is.na(is_fraud) & !is.na(amt) & is_fraud==0 ))
```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}

card_fraud %>% 
  #Filter out transactions without a fraud flag or amt
  filter(!is.na(is_fraud) & !is.na(amt)) %>% 
  #Group by transaction year
  group_by(category ) %>% 
  #summaries total amount for non-fraudulent, fraudulent transactions, and the % share for fraudulent transactions in term sof value
  summarise(Total_Fraudulent = sum(is_fraud),
            Total_Non_Fraudulent = length(is_fraud) -sum(is_fraud),
            percent_fraud= Total_Fraudulent/(Total_Non_Fraudulent+Total_Fraudulent)) %>% 
  #Order by most to least likely for fraudulent transaction
  mutate(category =fct_rev(fct_reorder(category,percent_fraud))) %>% 
  #bar chart plot
  ggplot(aes(x=category, percent_fraud)) +
  #plot on a barchart
  geom_col() +
  scale_y_continuous(labels = scales::percent_format()) 

 

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

```         
mutate(
  date_only = lubridate::date(trans_date_trans_time),
  month_name = lubridate::month(trans_date_trans_time, label=TRUE),
  hour = lubridate::hour(trans_date_trans_time),
  weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
  )
```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code

```         
  mutate(
   age = interval(dob, trans_date_trans_time) / years(1),
    )
```

```{r}

```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value)
```

Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdom? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below



```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "electricity-co2-gdp.png"), error = FALSE)
```


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (qmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be comitting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
